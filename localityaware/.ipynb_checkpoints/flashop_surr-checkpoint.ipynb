{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: C:\\Users\\ppromte1\\OneDrive - Johns Hopkins\\Process Design\\Aspen\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(parent_dir)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFlashOperation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mRefrig2DrumHeatExConstr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive - Johns Hopkins\\Process Design\\Aspen\\localityaware\\module.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cKDTree\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Get the absolute path of the notebook's directory\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Navigate to the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "\n",
    "# Add the parent directory to sys.path so we can import modules\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Verify the path\n",
    "print(f\"Added to sys.path: {parent_dir}\")\n",
    "\n",
    "# Add it to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from FlashOperation.Refrig2DrumHeatExConstr import *\n",
    "from module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Dummy Mode: Skipping Aspen simulation initialization.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "flash_1_range = np.linspace(6, 12, 20)\n",
    "flash_2_range = np.linspace(0.5, 4, 20)\n",
    "\n",
    "assSim = Refrig2DrumConstraintHeatExConstr(AspenFile = \"./FlashOperation/FlashOperation.bkp\", \n",
    "                                   wdpath = \"./FlashOperation\", \n",
    "                                   visibility=False,\n",
    "                                   Penalty=1e3\n",
    "                                   )\n",
    "\n",
    "data = []\n",
    "for flash_1 in flash_1_range:\n",
    "    for flash_2 in flash_2_range:\n",
    "        x_unflat = assSim.unflatten_params([flash_1, flash_2])\n",
    "        data.append([flash_1, flash_2, assSim.run_obj(x_unflat)])\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def optimize_surrogate_model(\n",
    "    model, old_dataset, new_dataset, assSim, \n",
    "    optim_steps=40, N_s=5, lr=0.001, merge_interval=10,\n",
    "    x_init=torch.tensor([0.9, -0.9], dtype=torch.float32, requires_grad=True)\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs online optimization using a surrogate model.\n",
    "\n",
    "    Args:\n",
    "    - model: Neural network model (MLP instance)\n",
    "    - old_dataset (OldDataSet): Historical dataset\n",
    "    - new_dataset (NewDataSet): New data storage\n",
    "    - assSim: Object with run_obj() function for objective evaluation\n",
    "    - optim_steps (int): Number of optimization steps\n",
    "    - N_s (int): Number of local samples per step\n",
    "    - lr (float): Learning rate for x updates\n",
    "    - merge_interval (int): Steps after which new data merges with old dataset\n",
    "\n",
    "    Returns:\n",
    "    - x_path (list): History of `x` points explored\n",
    "    - y_path (list): Corresponding objective function values\n",
    "    \"\"\"\n",
    "\n",
    "    # **Initialize Optimization Variables** \n",
    "    optimizer = torch.optim.Adam([x_init], lr=lr)\n",
    "\n",
    "    x_path, y_path = [], []\n",
    "\n",
    "    # **Optimization Loop**\n",
    "    for step in range(optim_steps):\n",
    "        print(f\"\\nStep {step + 1}/{optim_steps}\")\n",
    "\n",
    "        new_samples = []\n",
    "        \n",
    "        # Generate new local samples\n",
    "        for _ in range(N_s):\n",
    "            x = x_init.detach() + torch.randn_like(x_init) * 0.1\n",
    "            y = assSim.run_obj(assSim.unflatten_params(x))\n",
    "            new_samples.append(torch.cat((x, y.unsqueeze(0))).numpy())\n",
    "\n",
    "        # Add new samples to `NewDataSet`\n",
    "        new_dataset.add_samples(np.stack(new_samples))\n",
    "\n",
    "        # Train surrogate model\n",
    "        model = train_model(model, old_dataset, new_dataset)\n",
    "\n",
    "        # Merge new samples into old dataset every `merge_interval` steps\n",
    "        if step % merge_interval == 0:\n",
    "            old_dataset.merge(new_dataset.data)\n",
    "            new_dataset.clear()\n",
    "\n",
    "        # **Select Best Sample from Generated Samples**\n",
    "        new_samples_array = np.stack(new_samples)\n",
    "        best_sample_idx = np.argmin(new_samples_array[:, -1])  # Minimize objective function\n",
    "        x_init = torch.tensor(new_samples_array[best_sample_idx][:2], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # **Track Optimization Path**\n",
    "        x_path.append(x_init.tolist())\n",
    "        y_path.append(new_samples_array[best_sample_idx, -1])\n",
    "\n",
    "        # **Gradient-Based Update Using Surrogate Model**\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_init)\n",
    "        grad = torch.autograd.grad(y_pred, x_init)[0]\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Updated x: {x_init.tolist()}, Function Value: {y_path[-1]}, Gradient: {grad.tolist()}\")\n",
    "\n",
    "    return x_path, y_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1/40\n",
      "Epoch 0: Total Loss=6.1124\n",
      "Epoch 1: Total Loss=7.1633\n",
      "Epoch 2: Total Loss=6.2287\n",
      "Epoch 3: Total Loss=5.8909\n",
      "Epoch 4: Total Loss=6.9220\n",
      "Epoch 5: Total Loss=6.0147\n",
      "Epoch 6: Total Loss=5.6922\n",
      "Epoch 7: Total Loss=6.7033\n",
      "Epoch 8: Total Loss=5.8155\n",
      "Epoch 9: Total Loss=5.5083\n",
      "Updated x: [0.9160996675491333, -0.7360634207725525], Function Value: 1.3810279369354248, Gradient: [0.024571888148784637, -0.04308870807290077]\n",
      "\n",
      "Step 2/40\n",
      "Epoch 0: Total Loss=4.1546\n",
      "Epoch 1: Total Loss=3.9496\n",
      "Epoch 2: Total Loss=3.4490\n",
      "Epoch 3: Total Loss=4.0270\n",
      "Epoch 4: Total Loss=3.8213\n",
      "Epoch 5: Total Loss=3.3392\n",
      "Epoch 6: Total Loss=3.8946\n",
      "Epoch 7: Total Loss=3.6885\n",
      "Epoch 8: Total Loss=3.2231\n",
      "Epoch 9: Total Loss=3.7545\n",
      "Updated x: [0.8062655925750732, -0.7122756838798523], Function Value: 1.1574008464813232, Gradient: [0.05587337538599968, -0.17038433253765106]\n",
      "\n",
      "Step 3/40\n",
      "Epoch 0: Total Loss=0.3154\n",
      "Epoch 1: Total Loss=0.2205\n",
      "Epoch 2: Total Loss=0.3433\n",
      "Epoch 3: Total Loss=0.2856\n",
      "Epoch 4: Total Loss=0.3104\n",
      "Epoch 5: Total Loss=0.3101\n",
      "Epoch 6: Total Loss=0.2147\n",
      "Epoch 7: Total Loss=0.3384\n",
      "Epoch 8: Total Loss=0.2833\n",
      "Epoch 9: Total Loss=0.3057\n",
      "Updated x: [0.661819577217102, -0.6540660262107849], Function Value: 0.8658075332641602, Gradient: [0.07774915546178818, -0.18919013440608978]\n",
      "\n",
      "Step 4/40\n",
      "Epoch 0: Total Loss=0.1936\n",
      "Epoch 1: Total Loss=0.2667\n",
      "Epoch 2: Total Loss=0.3797\n",
      "Epoch 3: Total Loss=0.1692\n",
      "Epoch 4: Total Loss=0.1941\n",
      "Epoch 5: Total Loss=0.2670\n",
      "Epoch 6: Total Loss=0.3736\n",
      "Epoch 7: Total Loss=0.1668\n",
      "Epoch 8: Total Loss=0.1918\n",
      "Epoch 9: Total Loss=0.2659\n",
      "Updated x: [0.6740880012512207, -0.4249117374420166], Function Value: 0.6349446177482605, Gradient: [0.06805668771266937, -0.1591930389404297]\n",
      "\n",
      "Step 5/40\n",
      "Epoch 0: Total Loss=0.2684\n",
      "Epoch 1: Total Loss=0.2317\n",
      "Epoch 2: Total Loss=0.2848\n",
      "Epoch 3: Total Loss=0.2162\n",
      "Epoch 4: Total Loss=0.2182\n",
      "Epoch 5: Total Loss=0.2652\n",
      "Epoch 6: Total Loss=0.2314\n",
      "Epoch 7: Total Loss=0.2844\n",
      "Epoch 8: Total Loss=0.2156\n",
      "Epoch 9: Total Loss=0.2189\n",
      "Updated x: [0.640200138092041, -0.2436738759279251], Function Value: 0.4692331850528717, Gradient: [0.0373833030462265, -0.04047905281186104]\n",
      "\n",
      "Step 6/40\n",
      "Epoch 0: Total Loss=0.2949\n",
      "Epoch 1: Total Loss=0.1709\n",
      "Epoch 2: Total Loss=0.2516\n",
      "Epoch 3: Total Loss=0.1826\n",
      "Epoch 4: Total Loss=0.1573\n",
      "Epoch 5: Total Loss=0.2191\n",
      "Epoch 6: Total Loss=0.1971\n",
      "Epoch 7: Total Loss=0.2641\n",
      "Epoch 8: Total Loss=0.1988\n",
      "Epoch 9: Total Loss=0.1825\n",
      "Updated x: [0.6018614172935486, -0.10971197485923767], Function Value: 0.3742738664150238, Gradient: [0.01564641483128071, 0.05992020294070244]\n",
      "\n",
      "Step 7/40\n",
      "Epoch 0: Total Loss=0.1634\n",
      "Epoch 1: Total Loss=0.2342\n",
      "Epoch 2: Total Loss=0.1720\n",
      "Epoch 3: Total Loss=0.2944\n",
      "Epoch 4: Total Loss=0.1163\n",
      "Epoch 5: Total Loss=0.1805\n",
      "Epoch 6: Total Loss=0.1845\n",
      "Epoch 7: Total Loss=0.1672\n",
      "Epoch 8: Total Loss=0.1381\n",
      "Epoch 9: Total Loss=0.2575\n",
      "Updated x: [0.44358402490615845, 0.016260623931884766], Function Value: 0.1970311999320984, Gradient: [0.006917777005583048, 0.06825610995292664]\n",
      "\n",
      "Step 8/40\n",
      "Epoch 0: Total Loss=0.1445\n",
      "Epoch 1: Total Loss=0.1697\n",
      "Epoch 2: Total Loss=0.2209\n",
      "Epoch 3: Total Loss=0.1984\n",
      "Epoch 4: Total Loss=0.1824\n",
      "Epoch 5: Total Loss=0.2216\n",
      "Epoch 6: Total Loss=0.1884\n",
      "Epoch 7: Total Loss=0.1311\n",
      "Epoch 8: Total Loss=0.1151\n",
      "Epoch 9: Total Loss=0.1464\n",
      "Updated x: [0.3403835594654083, -0.13535337150096893], Function Value: 0.13418149948120117, Gradient: [0.006622523069381714, 0.06975679844617844]\n",
      "\n",
      "Step 9/40\n",
      "Epoch 0: Total Loss=0.1219\n",
      "Epoch 1: Total Loss=0.1106\n",
      "Epoch 2: Total Loss=0.1307\n",
      "Epoch 3: Total Loss=0.1693\n",
      "Epoch 4: Total Loss=0.2305\n",
      "Epoch 5: Total Loss=0.1367\n",
      "Epoch 6: Total Loss=0.1838\n",
      "Epoch 7: Total Loss=0.1347\n",
      "Epoch 8: Total Loss=0.2105\n",
      "Epoch 9: Total Loss=0.1964\n",
      "Updated x: [0.1888793408870697, -0.13062146306037903], Function Value: 0.05273737385869026, Gradient: [0.0020713612902909517, 0.06448911875486374]\n",
      "\n",
      "Step 10/40\n",
      "Epoch 0: Total Loss=0.0950\n",
      "Epoch 1: Total Loss=0.1266\n",
      "Epoch 2: Total Loss=0.2329\n",
      "Epoch 3: Total Loss=0.1013\n",
      "Epoch 4: Total Loss=0.2036\n",
      "Epoch 5: Total Loss=0.2072\n",
      "Epoch 6: Total Loss=0.1404\n",
      "Epoch 7: Total Loss=0.1950\n",
      "Epoch 8: Total Loss=0.1063\n",
      "Epoch 9: Total Loss=0.1314\n",
      "Updated x: [0.09372175484895706, -0.06332295387983322], Function Value: 0.012793563306331635, Gradient: [-0.00033725163666531444, 0.06383182853460312]\n",
      "\n",
      "Step 11/40\n",
      "Epoch 0: Total Loss=0.0761\n",
      "Epoch 1: Total Loss=0.0879\n",
      "Epoch 2: Total Loss=0.1058\n",
      "Epoch 3: Total Loss=0.1538\n",
      "Epoch 4: Total Loss=0.1331\n",
      "Epoch 5: Total Loss=0.1560\n",
      "Epoch 6: Total Loss=0.1547\n",
      "Epoch 7: Total Loss=0.1061\n",
      "Epoch 8: Total Loss=0.2583\n",
      "Epoch 9: Total Loss=0.1819\n",
      "Updated x: [0.09494531899690628, -0.09529930353164673], Function Value: 0.018096569925546646, Gradient: [0.0005377875058911741, 0.04271279647946358]\n",
      "\n",
      "Step 12/40\n",
      "Epoch 0: Total Loss=0.0663\n",
      "Epoch 1: Total Loss=0.0660\n",
      "Epoch 2: Total Loss=0.0620\n",
      "Epoch 3: Total Loss=0.0656\n",
      "Epoch 4: Total Loss=0.0651\n",
      "Epoch 5: Total Loss=0.0618\n",
      "Epoch 6: Total Loss=0.0652\n",
      "Epoch 7: Total Loss=0.0649\n",
      "Epoch 8: Total Loss=0.0614\n",
      "Epoch 9: Total Loss=0.0648\n",
      "Updated x: [-0.01096537709236145, -0.02299310266971588], Function Value: 0.0006489222869277, Gradient: [-0.002117787254974246, 0.034650832414627075]\n",
      "\n",
      "Step 13/40\n",
      "Epoch 0: Total Loss=0.0654\n",
      "Epoch 1: Total Loss=0.0622\n",
      "Epoch 2: Total Loss=0.0624\n",
      "Epoch 3: Total Loss=0.0648\n",
      "Epoch 4: Total Loss=0.0626\n",
      "Epoch 5: Total Loss=0.0643\n",
      "Epoch 6: Total Loss=0.0623\n",
      "Epoch 7: Total Loss=0.0623\n",
      "Epoch 8: Total Loss=0.0645\n",
      "Epoch 9: Total Loss=0.0619\n",
      "Updated x: [-0.005180089268833399, 0.004011055454611778], Function Value: 4.2921892600134015e-05, Gradient: [-0.0006996922311373055, 0.01569526270031929]\n",
      "\n",
      "Step 14/40\n",
      "Epoch 0: Total Loss=0.0635\n",
      "Epoch 1: Total Loss=0.0611\n",
      "Epoch 2: Total Loss=0.0613\n",
      "Epoch 3: Total Loss=0.0615\n",
      "Epoch 4: Total Loss=0.0631\n",
      "Epoch 5: Total Loss=0.0613\n",
      "Epoch 6: Total Loss=0.0615\n",
      "Epoch 7: Total Loss=0.0609\n",
      "Epoch 8: Total Loss=0.0637\n",
      "Epoch 9: Total Loss=0.0614\n",
      "Updated x: [0.08385103195905685, 0.03837994486093521], Function Value: 0.008504015393555164, Gradient: [0.0011205750051885843, 0.015210442245006561]\n",
      "\n",
      "Step 15/40\n",
      "Epoch 0: Total Loss=0.0621\n",
      "Epoch 1: Total Loss=0.0612\n",
      "Epoch 2: Total Loss=0.0637\n",
      "Epoch 3: Total Loss=0.0633\n",
      "Epoch 4: Total Loss=0.0608\n",
      "Epoch 5: Total Loss=0.0619\n",
      "Epoch 6: Total Loss=0.0613\n",
      "Epoch 7: Total Loss=0.0641\n",
      "Epoch 8: Total Loss=0.0631\n",
      "Epoch 9: Total Loss=0.0616\n",
      "Updated x: [-0.023456797003746033, 0.03469439223408699], Function Value: 0.0017539222026243806, Gradient: [0.0009545322391204536, 0.01175845880061388]\n",
      "\n",
      "Step 16/40\n",
      "Epoch 0: Total Loss=0.0607\n",
      "Epoch 1: Total Loss=0.0617\n",
      "Epoch 2: Total Loss=0.0614\n",
      "Epoch 3: Total Loss=0.0634\n",
      "Epoch 4: Total Loss=0.0611\n",
      "Epoch 5: Total Loss=0.0635\n",
      "Epoch 6: Total Loss=0.0603\n",
      "Epoch 7: Total Loss=0.0614\n",
      "Epoch 8: Total Loss=0.0623\n",
      "Epoch 9: Total Loss=0.0610\n",
      "Updated x: [0.005605621263384819, -0.05835247412323952], Function Value: 0.0034364343155175447, Gradient: [-0.0008198024588637054, 0.012203301303088665]\n",
      "\n",
      "Step 17/40\n",
      "Epoch 0: Total Loss=0.0607\n",
      "Epoch 1: Total Loss=0.0638\n",
      "Epoch 2: Total Loss=0.0610\n",
      "Epoch 3: Total Loss=0.0620\n",
      "Epoch 4: Total Loss=0.0607\n",
      "Epoch 5: Total Loss=0.0634\n",
      "Epoch 6: Total Loss=0.0611\n",
      "Epoch 7: Total Loss=0.0608\n",
      "Epoch 8: Total Loss=0.0631\n",
      "Epoch 9: Total Loss=0.0605\n",
      "Updated x: [-0.015609053894877434, 0.01427522674202919], Function Value: 0.0004474246525205672, Gradient: [0.001592613640241325, 0.011823571287095547]\n",
      "\n",
      "Step 18/40\n",
      "Epoch 0: Total Loss=0.0622\n",
      "Epoch 1: Total Loss=0.0608\n",
      "Epoch 2: Total Loss=0.0609\n",
      "Epoch 3: Total Loss=0.0619\n",
      "Epoch 4: Total Loss=0.0616\n",
      "Epoch 5: Total Loss=0.0610\n",
      "Epoch 6: Total Loss=0.0610\n",
      "Epoch 7: Total Loss=0.0631\n",
      "Epoch 8: Total Loss=0.0654\n",
      "Epoch 9: Total Loss=0.0625\n",
      "Updated x: [0.021178917959332466, -0.020520519465208054], Function Value: 0.0008696382865309715, Gradient: [0.001527351443655789, 0.01198351476341486]\n",
      "\n",
      "Step 19/40\n",
      "Epoch 0: Total Loss=0.0629\n",
      "Epoch 1: Total Loss=0.0610\n",
      "Epoch 2: Total Loss=0.0609\n",
      "Epoch 3: Total Loss=0.0625\n",
      "Epoch 4: Total Loss=0.0620\n",
      "Epoch 5: Total Loss=0.0608\n",
      "Epoch 6: Total Loss=0.0614\n",
      "Epoch 7: Total Loss=0.0611\n",
      "Epoch 8: Total Loss=0.0623\n",
      "Epoch 9: Total Loss=0.0609\n",
      "Updated x: [0.012881018221378326, 0.0016510188579559326], Function Value: 0.00016864649660419673, Gradient: [0.0027130490634590387, 0.01083260029554367]\n",
      "\n",
      "Step 20/40\n",
      "Epoch 0: Total Loss=0.0614\n",
      "Epoch 1: Total Loss=0.0606\n",
      "Epoch 2: Total Loss=0.0611\n",
      "Epoch 3: Total Loss=0.0633\n",
      "Epoch 4: Total Loss=0.0605\n",
      "Epoch 5: Total Loss=0.0622\n",
      "Epoch 6: Total Loss=0.0625\n",
      "Epoch 7: Total Loss=0.0609\n",
      "Epoch 8: Total Loss=0.0610\n",
      "Epoch 9: Total Loss=0.0629\n",
      "Updated x: [0.06209368258714676, 0.04013892263174057], Function Value: 0.005466758273541927, Gradient: [0.004176380578428507, 0.010386201553046703]\n",
      "\n",
      "Step 21/40\n",
      "Epoch 0: Total Loss=0.0617\n",
      "Epoch 1: Total Loss=0.0608\n",
      "Epoch 2: Total Loss=0.0603\n",
      "Epoch 3: Total Loss=0.0611\n",
      "Epoch 4: Total Loss=0.0625\n",
      "Epoch 5: Total Loss=0.0604\n",
      "Epoch 6: Total Loss=0.0626\n",
      "Epoch 7: Total Loss=0.0610\n",
      "Epoch 8: Total Loss=0.0610\n",
      "Epoch 9: Total Loss=0.0624\n",
      "Updated x: [-0.05250124633312225, -0.03732828050851822], Function Value: 0.0041497815400362015, Gradient: [0.001016508787870407, 0.008497663773596287]\n",
      "\n",
      "Step 22/40\n",
      "Epoch 0: Total Loss=0.0591\n",
      "Epoch 1: Total Loss=0.0591\n",
      "Epoch 2: Total Loss=0.0541\n",
      "Epoch 3: Total Loss=0.0589\n",
      "Epoch 4: Total Loss=0.0590\n",
      "Epoch 5: Total Loss=0.0540\n",
      "Epoch 6: Total Loss=0.0589\n",
      "Epoch 7: Total Loss=0.0590\n",
      "Epoch 8: Total Loss=0.0540\n",
      "Epoch 9: Total Loss=0.0588\n",
      "Updated x: [-0.04873272031545639, 0.024549048393964767], Function Value: 0.002977533731609583, Gradient: [2.1250640202197246e-05, -0.0004875848244410008]\n",
      "\n",
      "Step 23/40\n",
      "Epoch 0: Total Loss=0.0582\n",
      "Epoch 1: Total Loss=0.0568\n",
      "Epoch 2: Total Loss=0.0594\n",
      "Epoch 3: Total Loss=0.0570\n",
      "Epoch 4: Total Loss=0.0584\n",
      "Epoch 5: Total Loss=0.0582\n",
      "Epoch 6: Total Loss=0.0568\n",
      "Epoch 7: Total Loss=0.0594\n",
      "Epoch 8: Total Loss=0.0570\n",
      "Epoch 9: Total Loss=0.0584\n",
      "Updated x: [-0.036805346608161926, 0.10583755373954773], Function Value: 0.012556221336126328, Gradient: [0.00014049408491700888, -0.00079112418461591]\n",
      "\n",
      "Step 24/40\n",
      "Epoch 0: Total Loss=0.0587\n",
      "Epoch 1: Total Loss=0.0576\n",
      "Epoch 2: Total Loss=0.0575\n",
      "Epoch 3: Total Loss=0.0561\n",
      "Epoch 4: Total Loss=0.0586\n",
      "Epoch 5: Total Loss=0.0576\n",
      "Epoch 6: Total Loss=0.0575\n",
      "Epoch 7: Total Loss=0.0560\n",
      "Epoch 8: Total Loss=0.0586\n",
      "Epoch 9: Total Loss=0.0575\n",
      "Updated x: [0.013499375432729721, 0.0466180145740509], Function Value: 0.002355472417548299, Gradient: [-0.0005405236152000725, 0.006025243550539017]\n",
      "\n",
      "Step 25/40\n",
      "Epoch 0: Total Loss=0.0587\n",
      "Epoch 1: Total Loss=0.0555\n",
      "Epoch 2: Total Loss=0.0579\n",
      "Epoch 3: Total Loss=0.0571\n",
      "Epoch 4: Total Loss=0.0559\n",
      "Epoch 5: Total Loss=0.0586\n",
      "Epoch 6: Total Loss=0.0555\n",
      "Epoch 7: Total Loss=0.0578\n",
      "Epoch 8: Total Loss=0.0572\n",
      "Epoch 9: Total Loss=0.0558\n",
      "Updated x: [-0.03150390088558197, 0.009577326476573944], Function Value: 0.001084220944903791, Gradient: [-0.0017687543295323849, 0.007473093923181295]\n",
      "\n",
      "Step 26/40\n",
      "Epoch 0: Total Loss=0.0548\n",
      "Epoch 1: Total Loss=0.0554\n",
      "Epoch 2: Total Loss=0.0554\n",
      "Epoch 3: Total Loss=0.0570\n",
      "Epoch 4: Total Loss=0.0598\n",
      "Epoch 5: Total Loss=0.0558\n",
      "Epoch 6: Total Loss=0.0547\n",
      "Epoch 7: Total Loss=0.0547\n",
      "Epoch 8: Total Loss=0.0552\n",
      "Epoch 9: Total Loss=0.0555\n",
      "Updated x: [-0.04189835488796234, -0.004797029308974743], Function Value: 0.0017784836236387491, Gradient: [-0.0018846337916329503, 0.007275309879332781]\n",
      "\n",
      "Step 27/40\n",
      "Epoch 0: Total Loss=0.0584\n",
      "Epoch 1: Total Loss=0.0539\n",
      "Epoch 2: Total Loss=0.0582\n",
      "Epoch 3: Total Loss=0.0551\n",
      "Epoch 4: Total Loss=0.0547\n",
      "Epoch 5: Total Loss=0.0556\n",
      "Epoch 6: Total Loss=0.0557\n",
      "Epoch 7: Total Loss=0.0545\n",
      "Epoch 8: Total Loss=0.0584\n",
      "Epoch 9: Total Loss=0.0548\n",
      "Updated x: [0.04881565272808075, -0.029817938804626465], Function Value: 0.003272077301517129, Gradient: [-0.001377451466396451, 0.00508294440805912]\n",
      "\n",
      "Step 28/40\n",
      "Epoch 0: Total Loss=0.0579\n",
      "Epoch 1: Total Loss=0.0571\n",
      "Epoch 2: Total Loss=0.0544\n",
      "Epoch 3: Total Loss=0.0561\n",
      "Epoch 4: Total Loss=0.0553\n",
      "Epoch 5: Total Loss=0.0540\n",
      "Epoch 6: Total Loss=0.0542\n",
      "Epoch 7: Total Loss=0.0542\n",
      "Epoch 8: Total Loss=0.0587\n",
      "Epoch 9: Total Loss=0.0577\n",
      "Updated x: [-0.0003362782299518585, 0.0011968258768320084], Function Value: 1.5454752428922802e-06, Gradient: [-0.0007901982171460986, 0.008419571444392204]\n",
      "\n",
      "Step 29/40\n",
      "Epoch 0: Total Loss=0.0541\n",
      "Epoch 1: Total Loss=0.0545\n",
      "Epoch 2: Total Loss=0.0592\n",
      "Epoch 3: Total Loss=0.0540\n",
      "Epoch 4: Total Loss=0.0550\n",
      "Epoch 5: Total Loss=0.0543\n",
      "Epoch 6: Total Loss=0.0549\n",
      "Epoch 7: Total Loss=0.0576\n",
      "Epoch 8: Total Loss=0.0541\n",
      "Epoch 9: Total Loss=0.0552\n",
      "Updated x: [0.017583148553967476, -0.03417804837226868], Function Value: 0.0014773061266168952, Gradient: [-0.0005796775803901255, 0.006919748615473509]\n",
      "\n",
      "Step 30/40\n",
      "Epoch 0: Total Loss=0.0543\n",
      "Epoch 1: Total Loss=0.0548\n",
      "Epoch 2: Total Loss=0.0547\n",
      "Epoch 3: Total Loss=0.0550\n",
      "Epoch 4: Total Loss=0.0542\n",
      "Epoch 5: Total Loss=0.0544\n",
      "Epoch 6: Total Loss=0.0563\n",
      "Epoch 7: Total Loss=0.0546\n",
      "Epoch 8: Total Loss=0.0570\n",
      "Epoch 9: Total Loss=0.0538\n",
      "Updated x: [-0.03536296635866165, 0.03643286973237991], Function Value: 0.0025778934359550476, Gradient: [0.00017548502364661545, 0.004534516017884016]\n",
      "\n",
      "Step 31/40\n",
      "Epoch 0: Total Loss=0.0551\n",
      "Epoch 1: Total Loss=0.0574\n",
      "Epoch 2: Total Loss=0.0539\n",
      "Epoch 3: Total Loss=0.0540\n",
      "Epoch 4: Total Loss=0.0540\n",
      "Epoch 5: Total Loss=0.0542\n",
      "Epoch 6: Total Loss=0.0546\n",
      "Epoch 7: Total Loss=0.0541\n",
      "Epoch 8: Total Loss=0.0571\n",
      "Epoch 9: Total Loss=0.0565\n",
      "Updated x: [-0.0339549221098423, 0.03932761028409004], Function Value: 0.0026995977386832237, Gradient: [0.0008527578902430832, 0.005693922750651836]\n",
      "\n",
      "Step 32/40\n",
      "Epoch 0: Total Loss=0.0689\n",
      "Epoch 1: Total Loss=0.0745\n",
      "Epoch 2: Total Loss=0.0735\n",
      "Epoch 3: Total Loss=0.0778\n",
      "Epoch 4: Total Loss=0.0763\n",
      "Epoch 5: Total Loss=0.0766\n",
      "Epoch 6: Total Loss=0.0770\n",
      "Epoch 7: Total Loss=0.0706\n",
      "Epoch 8: Total Loss=0.0787\n",
      "Epoch 9: Total Loss=0.0745\n",
      "Updated x: [-0.04573856294155121, 0.1135760247707367], Function Value: 0.014991529285907745, Gradient: [0.0003404443268664181, 0.004878441337496042]\n",
      "\n",
      "Step 33/40\n",
      "Epoch 0: Total Loss=0.0728\n",
      "Epoch 1: Total Loss=0.0756\n",
      "Epoch 2: Total Loss=0.0759\n",
      "Epoch 3: Total Loss=0.0818\n",
      "Epoch 4: Total Loss=0.0783\n",
      "Epoch 5: Total Loss=0.0763\n",
      "Epoch 6: Total Loss=0.0767\n",
      "Epoch 7: Total Loss=0.0797\n",
      "Epoch 8: Total Loss=0.0773\n",
      "Epoch 9: Total Loss=0.0807\n",
      "Updated x: [-0.04215463250875473, -0.06300093233585358], Function Value: 0.005746130831539631, Gradient: [-0.0014196204720064998, 0.014025921933352947]\n",
      "\n",
      "Step 34/40\n",
      "Epoch 0: Total Loss=0.0722\n",
      "Epoch 1: Total Loss=0.0758\n",
      "Epoch 2: Total Loss=0.0803\n",
      "Epoch 3: Total Loss=0.0755\n",
      "Epoch 4: Total Loss=0.0772\n",
      "Epoch 5: Total Loss=0.0710\n",
      "Epoch 6: Total Loss=0.0663\n",
      "Epoch 7: Total Loss=0.0726\n",
      "Epoch 8: Total Loss=0.0770\n",
      "Epoch 9: Total Loss=0.0720\n",
      "Updated x: [0.049727581441402435, -0.05934959277510643], Function Value: 0.005995206534862518, Gradient: [-0.0006323016132228076, 0.0170072503387928]\n",
      "\n",
      "Step 35/40\n",
      "Epoch 0: Total Loss=0.0824\n",
      "Epoch 1: Total Loss=0.0856\n",
      "Epoch 2: Total Loss=0.0823\n",
      "Epoch 3: Total Loss=0.0726\n",
      "Epoch 4: Total Loss=0.0766\n",
      "Epoch 5: Total Loss=0.0742\n",
      "Epoch 6: Total Loss=0.0747\n",
      "Epoch 7: Total Loss=0.0831\n",
      "Epoch 8: Total Loss=0.0765\n",
      "Epoch 9: Total Loss=0.0743\n",
      "Updated x: [-0.0017480924725532532, -0.0039130039513111115], Function Value: 1.836742740124464e-05, Gradient: [0.00011658661242108792, 0.013148886151611805]\n",
      "\n",
      "Step 36/40\n",
      "Epoch 0: Total Loss=0.0795\n",
      "Epoch 1: Total Loss=0.0755\n",
      "Epoch 2: Total Loss=0.0745\n",
      "Epoch 3: Total Loss=0.0749\n",
      "Epoch 4: Total Loss=0.0763\n",
      "Epoch 5: Total Loss=0.0788\n",
      "Epoch 6: Total Loss=0.0740\n",
      "Epoch 7: Total Loss=0.0848\n",
      "Epoch 8: Total Loss=0.0753\n",
      "Epoch 9: Total Loss=0.0699\n",
      "Updated x: [0.01867864280939102, 0.006062728352844715], Function Value: 0.00038564836722798645, Gradient: [4.836876905756071e-05, 0.007110681384801865]\n",
      "\n",
      "Step 37/40\n",
      "Epoch 0: Total Loss=0.0726\n",
      "Epoch 1: Total Loss=0.0749\n",
      "Epoch 2: Total Loss=0.0689\n",
      "Epoch 3: Total Loss=0.0777\n",
      "Epoch 4: Total Loss=0.0767\n",
      "Epoch 5: Total Loss=0.0786\n",
      "Epoch 6: Total Loss=0.0700\n",
      "Epoch 7: Total Loss=0.0786\n",
      "Epoch 8: Total Loss=0.0728\n",
      "Epoch 9: Total Loss=0.0720\n",
      "Updated x: [-0.07016905397176743, -0.002270958386361599], Function Value: 0.004928853362798691, Gradient: [0.0005177483544684947, 0.011835387907922268]\n",
      "\n",
      "Step 38/40\n",
      "Epoch 0: Total Loss=0.0796\n",
      "Epoch 1: Total Loss=0.0734\n",
      "Epoch 2: Total Loss=0.0732\n",
      "Epoch 3: Total Loss=0.0698\n",
      "Epoch 4: Total Loss=0.0739\n",
      "Epoch 5: Total Loss=0.0734\n",
      "Epoch 6: Total Loss=0.0743\n",
      "Epoch 7: Total Loss=0.0717\n",
      "Epoch 8: Total Loss=0.0805\n",
      "Epoch 9: Total Loss=0.0752\n",
      "Updated x: [0.030769824981689453, -0.04456629976630211], Function Value: 0.002932937117293477, Gradient: [0.0008965187589637935, 0.014629763551056385]\n",
      "\n",
      "Step 39/40\n",
      "Epoch 0: Total Loss=0.0734\n",
      "Epoch 1: Total Loss=0.0787\n",
      "Epoch 2: Total Loss=0.0731\n",
      "Epoch 3: Total Loss=0.0794\n",
      "Epoch 4: Total Loss=0.0770\n",
      "Epoch 5: Total Loss=0.0744\n",
      "Epoch 6: Total Loss=0.0769\n",
      "Epoch 7: Total Loss=0.0763\n",
      "Epoch 8: Total Loss=0.0723\n",
      "Epoch 9: Total Loss=0.0742\n",
      "Updated x: [0.02311529591679573, 0.015005599707365036], Function Value: 0.0007594849448651075, Gradient: [0.0012272924650460482, 0.004527338780462742]\n",
      "\n",
      "Step 40/40\n",
      "Epoch 0: Total Loss=0.0731\n",
      "Epoch 1: Total Loss=0.0782\n",
      "Epoch 2: Total Loss=0.0740\n",
      "Epoch 3: Total Loss=0.0710\n",
      "Epoch 4: Total Loss=0.0780\n",
      "Epoch 5: Total Loss=0.0754\n",
      "Epoch 6: Total Loss=0.0795\n",
      "Epoch 7: Total Loss=0.0726\n",
      "Epoch 8: Total Loss=0.0714\n",
      "Epoch 9: Total Loss=0.0776\n",
      "Updated x: [-0.019815601408481598, -0.01730206236243248], Function Value: 0.000692019413691014, Gradient: [0.001163544482551515, 0.008098499849438667]\n"
     ]
    }
   ],
   "source": [
    "# **Initialize Model & Datasets**\n",
    "old_dataset = OldDataSet(data, k=10)\n",
    "new_dataset = NewDataSet(k=10)\n",
    "model = MLP(2, [5, 5, 5], 1)\n",
    "\n",
    "# **Run Optimization**\n",
    "x_path, y_path = optimize_surrogate_model(model, old_dataset, new_dataset, assSim, optim_steps=40, N_s=5, lr=0.001, merge_interval=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
